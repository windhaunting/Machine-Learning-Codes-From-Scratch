{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T21:23:03.743457Z","iopub.execute_input":"2021-07-29T21:23:03.744233Z","iopub.status.idle":"2021-07-29T21:23:03.760066Z","shell.execute_reply.started":"2021-07-29T21:23:03.744053Z","shell.execute_reply":"2021-07-29T21:23:03.758419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# linear regression\n\n# y = wX + b   => wX      w: D+1 dimension\n# use MSE/MAE   \n# L = ((y-y^hat))^2;   y^hat = wX\n# dL/dw = 1/N * 2(y - y_hat)*x, \n# w = w - r*dL/dw\n\n\nPros:\n\nLinear regression is straightforward to understand and explain, and can be regularized to avoid overfitting. In addition, linear models can be updated easily with new data using stochastic gradient descent.\n\n\nCons:\n\nLinear regression performs poorly when there are non-linear relationships. They are not naturally flexible enough to capture more complex patterns, and adding the right interaction terms or polynomials can be tricky and time-consuming.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom numpy.random import rand\nfrom sklearn import datasets\nfrom sklearn.metrics import precision_recall_fscore_support\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom sklearn.model_selection import train_test_split\nfrom copy import deepcopy","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:03.762434Z","iopub.execute_input":"2021-07-29T21:23:03.762989Z","iopub.status.idle":"2021-07-29T21:23:04.979116Z","shell.execute_reply.started":"2021-07-29T21:23:03.762932Z","shell.execute_reply":"2021-07-29T21:23:04.977956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearRegression(object):\n\n    def __init__(self):\n        self.w = None          #dimension D x 1         \n        self.loss = []\n        self.history_weights = []\n        \n    def linear_func(self, X, w):\n        return np.dot(X, w)\n    \n    def fit(self, X, y, epochs=20, lr=0.05):\n        # input: X -- NXD ; y -- NX1\n        # gradient descent\n                \n        w = rand(X.shape[1])   # weight: Dx1\n        b = rand(X.shape[1])   \n        \n        N = len(X)          # number of instances\n        for i in range(epochs):\n            \n            #Gradient descent\n            y_hat = self.linear_func(X, w)\n            #print(\"ss11111: \", X.shape, y.shape, w.shape, b.shape, y_hat.shape)\n           \n            dw = 2 * np.dot(X.T, y_hat - y)/N\n            \n            w -= lr * dw        # lr * dot(X.T, y_hat -y)/ N\n            \n            curr_loss = self.loss_func(X, y, w)\n            #print(\"wwwww: \", w)\n            self.loss.append(curr_loss)\n            \n            self.history_weights.append(deepcopy(w))\n            \n        self.w = w\n        self.b = b\n                                   \n    def loss_func(self, X, y_true, w):\n        # use MSE here\n        N = len(X)\n        y_hat = self.linear_func(X, w)\n        loss = 1/N*(np.sum(np.square((y_true - y_hat))))\n        #print(\"loss: \", loss)\n\n        return loss\n    \n    def predict(self, X):\n        return self.linear_func(X, self.w)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:04.981701Z","iopub.execute_input":"2021-07-29T21:23:04.982252Z","iopub.status.idle":"2021-07-29T21:23:04.994897Z","shell.execute_reply.started":"2021-07-29T21:23:04.982196Z","shell.execute_reply":"2021-07-29T21:23:04.993553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read data to train and test","metadata":{}},{"cell_type":"code","source":"# read data;  generate random data\nn_samples = 100\nX, y = datasets.make_regression(n_samples=n_samples, n_features=1,\n                                      n_informative=1, noise=10,\n                                      coef=False, random_state=0)\n\nprint (\"X:\", X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:04.996939Z","iopub.execute_input":"2021-07-29T21:23:04.99741Z","iopub.status.idle":"2021-07-29T21:23:05.017998Z","shell.execute_reply.started":"2021-07-29T21:23:04.997353Z","shell.execute_reply":"2021-07-29T21:23:05.016563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nLinearRegression_obj = LinearRegression()\nepochs = 50\nLinearRegression_obj.fit(X_train, y_train, epochs=epochs)\n\nprint(\"LinearRegression_obj.history_weights: \", LinearRegression_obj.history_weights)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:05.019499Z","iopub.execute_input":"2021-07-29T21:23:05.019826Z","iopub.status.idle":"2021-07-29T21:23:05.034465Z","shell.execute_reply.started":"2021-07-29T21:23:05.019781Z","shell.execute_reply":"2021-07-29T21:23:05.033493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ny_test_hat = LinearRegression_obj.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:05.035857Z","iopub.execute_input":"2021-07-29T21:23:05.036605Z","iopub.status.idle":"2021-07-29T21:23:05.040635Z","shell.execute_reply.started":"2021-07-29T21:23:05.036551Z","shell.execute_reply":"2021-07-29T21:23:05.039848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot\nplt.plot(LinearRegression_obj.loss)\nplt.show()\n#print(\"y: \", y)\n#print(\"y_test_hat: \", y_test_hat)\nprint(\"loss: \", LinearRegression_obj.loss)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:05.041919Z","iopub.execute_input":"2021-07-29T21:23:05.042427Z","iopub.status.idle":"2021-07-29T21:23:05.237471Z","shell.execute_reply.started":"2021-07-29T21:23:05.042383Z","shell.execute_reply":"2021-07-29T21:23:05.236292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Animation\n\n#Set the plot up,\nfig = plt.figure()\nax = plt.axes()\nplt.title('Sale Price vs Living Area')\nplt.xlabel('Living Area in square feet (normalised)')\nplt.ylabel('Sale Price ($)')\nplt.scatter(X_train, y_train, color='red')\nline, = ax.plot([], [], lw=2)\nannotation = ax.text(-1, 700000, '')\nannotation.set_animated(True)\nplt.close()\n\n#Generate the animation data,\ndef init():\n    line.set_data([], [])\n    annotation.set_text('')\n    return line, annotation\n\n# animation function.  This is called sequentially\ndef animate(i):\n    x = np.linspace(-5, 20, 100)\n    #print(\"LinearRegression_obj.history_weights[i]: \", LinearRegression_obj.history_weights[i])\n    y = LinearRegression_obj.history_weights[i][0]*x\n    line.set_data(x, y)\n    annotation.set_text('Cost = %.2f e10' % (LinearRegression_obj.loss[i]/10000000000))\n    return line, annotation\n\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=epochs, interval=0, blit=True)\n\nanim.save('animation.gif', writer='imagemagick', fps = 30)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:05.239311Z","iopub.execute_input":"2021-07-29T21:23:05.239661Z","iopub.status.idle":"2021-07-29T21:23:14.812711Z","shell.execute_reply.started":"2021-07-29T21:23:05.239628Z","shell.execute_reply":"2021-07-29T21:23:14.811515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display the animation...\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfilename = 'animation.gif'\n\nvideo = io.open(filename, 'r+b').read()\nencoded = base64.b64encode(video)\nHTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T21:23:14.815289Z","iopub.execute_input":"2021-07-29T21:23:14.81587Z","iopub.status.idle":"2021-07-29T21:23:14.839693Z","shell.execute_reply.started":"2021-07-29T21:23:14.815817Z","shell.execute_reply":"2021-07-29T21:23:14.83842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}